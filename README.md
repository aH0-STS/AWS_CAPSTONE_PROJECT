# AWS_Capstone_Project

# üåç Employee Manager service ‚Äì Multi-Region Deployment with Route 53 Failover

## 1. üßæ Introduction

This project sets up a resilient, production-ready, multi-region web application using a 3-tier architecture. It leverages **Amazon EKS** for container orchestration, **Amazon ECR** for image Storage, **Amazon RDS MySQL** for data persistence, and **Route 53** for intelligent global traffic management and failover.

Infrastructure is provisioned with **CloudFormation in Region 1** and **Terraform in Region 2**, demonstrating best practices for hybrid infrastructure-as-code (IaC) deployments. CI/CD pipelines automate both **frontend (Angular)** and **backend (Spring Boot)** deployments.

---

## 2. üì± Application Overview

| Component  | Tech Stack                        |
|------------|-----------------------------------|
| Frontend   | Angular (TypeScript)              |
| Backend    | Spring Boot (Java)                |
| Database   | Amazon RDS (MySQL)                |
| Infra      | CloudFormation (us-west-1), Terraform (us-west-2) |
| Orchestration | Amazon EKS                     |

---

## 3. üõ† Infrastructure Design Principles

| Goal              | Strategy                                                   |
|-------------------|------------------------------------------------------------|
| High Availability | Multi-AZ, Auto-scaling EKS nodes, Route 53 failover        |
| Fault Tolerance   | Redundant infrastructure across two regions                |
| Scalability       | Horizontal scaling via EKS node groups                     |
| DR Readiness      | Active-passive design with Route 53 and ALBs               |
| Automation        | Fully automated CI/CD pipelines for app deployment         |

---

## 4. üèóÔ∏è Overall Architecture

<img src="https://raw.githubusercontent.com/KIREETI1234/AWS_Capstone_Project/main/architecture.png" alt="Architecture Diagram" height="400" width="400"/>




## 5. üå© CloudFormation Deployment ‚Äì Region 1

**üìÅ File**: `infra/cloudformation/template.yaml`

### üîç Key Resources
- **Networking**: VPC, Public/Private Subnets, NAT Gateway, Route Tables
- **Compute**: EKS Cluster, Node Groups (Auto-scaling), IAM roles
- **Database**: RDS MySQL (Multi-AZ) with private access
- **Monitoring**: CloudWatch alarms, SNS topics
- **Security**: IAM roles for least privilege, SGs for EKS/RDS

### üõ† Deployment Methods
- **Manual**: Create a CloudFormation stack and upload `yaml` file
- **Automated**:
  1. Push template to GitHub
  2. Set up CodePipeline (source: GitHub, deploy: CloudFormation)
  3. Trigger deploy on push

---

## 6. ‚öôÔ∏è Terraform Deployment ‚Äì Region 2

**üìÅ Directory**: `infra/terraform/main.tf`

### üîß Modules & Resources
- `vpc`: Creates VPC and subnets
- `eks`: Deploys EKS cluster and node groups
- `rds`: Provisions MySQL DB (standby or DR)

### üõ† Steps to Deploy
```bash
cd infra/cloudformation/cft.yaml
terraform init
terraform validate
terraform plan
terraform apply

## 7. Global Traffic Management (Route 53)
 
### üß† Failover Logic
 
| Record Type | Region    | Failover Role | Health Check |

| ----------- | --------- | ------------- | ------------ |

| A (Alias)   | us-east-1 | PRIMARY       | Enabled      |

| A (Alias)   | us-west-2 | SECONDARY     | N/A          |
 
> If primary ALB is unhealthy, traffic fails over to secondary.
 
---
 
## 8. CI/CD Pipeline Setup
 
### üì¶ Tools Used
 
| Service      | Purpose                                  |

| ------------ | ---------------------------------------- |

| CodePipeline | Pipeline orchestration                   |

| CodeBuild    | Builds Spring Boot app & deploys to EKS  |

| CodeDeploy   | (Optional) EC2/ECS deployment management |

| ECR          | (Optional) Container image storage       |

| S3           | (Optional) Artifact storage              |
 
### üìä Architecture Diagram
 
```

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê

‚îÇ  GitHub    ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∂  ‚îÇ CodeBuild  ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∂  ‚îÇ EKS/Kubernetes‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ CodeDeploy ‚îÇ

‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

     ‚ñ≤                        ‚îÇ                   ‚îÇ                       ‚îÇ

     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ[Triggered on push]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ[Rolling updates / hooks]

```
 
### üß∞ Sample `buildspec.yml`
 
```yaml

version: 0.2

phases:

  install:

    runtime-versions:

      java: corretto11

  build:

    commands:

      - mvn clean package

      - kubectl apply -f deployment.yaml

artifacts:

  files: ["**/*"]

```
 
 
### üõ† CI/CD Setup Steps
 
1. Create CodePipeline with stages: Source (GitHub), Build (CodeBuild), Deploy (CodeBuild or CodeDeploy)

2. Store Docker image in ECR (if containerized)

3. Use IAM roles with EKS/CodeBuild/CodeDeploy permissions

4. Monitor via CloudWatch Logs
 
---
 
## 9. Monitoring & Alerting
 
### üîî Alarms
 
| Metric                 | Threshold | Action    |

| ---------------------- | --------- | --------- |

| RDS CPUUtilization     | >70%      | SNS Email |

| EKS Pod CPUUtilization | >80%      | SNS Email |
 
> Alerts delivered via SNS Topic (email notification)
 
---
 
## 10. Security Best Practices
 
* **Private RDS**: Not publicly accessible

* **IAM Roles**: Least privilege principle

* **Secrets**: Use SSM Parameter Store or Secrets Manager

* **Ingress Rules**: Restrict traffic using SGs and NACLs
 
---
 
## 11. Future Enhancements
 
* Add **WAF + Shield** for ALB

* Integrate **Prometheus + Grafana** for observability

* Extend CI/CD with canary/blue-green deployments
 
---
 
## 12. Author
 
**Arjun Koppineni**

GitHub: [@arjunkoppineni](https://github.com/arjunkoppineni)
 
---
 
## üìù Useful Commands
 
```bash

aws eks update-kubeconfig --region <region> --name <cluster-name>

kubectl get nodes

kubectl get svc

kubectl logs -l app=myapp

```
 
 


